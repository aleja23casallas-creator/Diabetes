# -*- coding: utf-8 -*-
"""Proyecto Entrega 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KuGCEkQzUpyxk6nJE2k5TqbylEbhblw3

##**Diabetes-Datos cl√≠nicos de 130 hospitales y centros de atenci√≥n integrados en EE.UU**

El conjunto de datos:
Readmisiones hospitalarias por diabetesde atenci√≥n cl√≠nica en 130 hospitales de EE. UU. y redes de prestaci√≥n integradas.
Periodo: 10 a√±os (1999‚Äì2008)


Los datos contienen atributos como el n√∫mero de pacientes, la raza, el sexo, la edad, el tipo de ingreso, el tiempo en el hospital, la especialidad m√©dica del m√©dico de admisi√≥n, el n√∫mero de pruebas de laboratorio realizadas, el resultado de la prueba de HbA1c, el diagn√≥stico, el n√∫mero de medicamentos, los medicamentos para la diabetes, el n√∫mero de visitas ambulatorias, hospitalarias y de emergencia en el a√±o anterior a la hospitalizaci√≥n, etc.

Objetivo: Analizar la readmisi√≥n de pacientes diab√©ticos, es decir, si despu√©s del alta regresan al hospital dentro de 30 d√≠as, despu√©s de 30 d√≠as o nunca

---
---
# **1. Obtener los datos**

---
---
"""

# =========================
# Importaciones EXACTAS que ten√≠as
# =========================
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from matplotlib import pyplot

# (ADAPTACI√ìN) Importaciones para Streamlit y gr√°ficos
import streamlit as st
import io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# =========================
# Mostrar el docstring inicial (tu explicaci√≥n original)
# =========================
if __doc__:
    st.markdown(__doc__)

# =========================
# (ADAPTACI√ìN) Colab: montar Drive (se omite fuera de Colab)
# =========================
try:
    from google.colab import drive
    drive.mount('/content/drive')
except Exception:
    st.info("Ejecutando fuera de Colab: se omite drive.mount().")

# =========================
# CARGA DE DATOS (ADAPTACI√ìN)
# Primero intenta leer del repo local; si falla, usa la ruta de Colab
# =========================
df = None
try:
    df = pd.read_csv("diabetic_data.csv")
except Exception:
    df = pd.read_csv("/content/drive/MyDrive/ML-BIOESTADISTICA/diabetic_data.csv")

st.write("Vista inicial del DataFrame:")
st.write(df.head())

# Ver valores √∫nicos en las columnas categ√≥ricas codificadas
st.write("admission_type_id:", sorted(df["admission_type_id"].unique()))
st.write("discharge_disposition_id:", sorted(df["discharge_disposition_id"].unique()))
st.write("admission_source_id:", sorted(df["admission_source_id"].unique()))

#Convertir a string
df["admission_type_id"] = df["admission_type_id"].astype(str)
df["discharge_disposition_id"] = df["discharge_disposition_id"].astype(str)
df["admission_source_id"] = df["admission_source_id"].astype(str)

map_admission_type = {
    "1": "Emergencia",
    "2": "Urgente",
    "3": "Electivo",
    "4": "Reci√©n nacido",
    "5": "Sin_info",
    "6": "Sin_info",
    "7": "Centro de trauma",
    "8": "Sin_info"
}

map_discharge_disposition = {
    "1": "Alta a casa",
    "2": "Alta a otro hospital a corto plazo",
    "3": "Alta a centro de enfermer√≠a especializada",
    "4": "Alta a centro de cuidados intermedios",
    "5": "Alta a otro tipo de atenci√≥n hospitalaria",
    "6": "Cuidados de salud en casa",
    "7": "Salida contra recomendaci√≥n m√©dica",
    "8": "Alta a casa con cuidados",
    "9": "Admitido como paciente hospitalizado",
    "10": "Cuidados paliativos en casa",
    "11": "Cuidados paliativos en centro m√©dico",
    "12": "Alta a hospital psiqui√°trico",
    "13": "Alta a otra instalaci√≥n de rehabilitaci√≥n",
    "14": "Sin_info",
    "15": "Sin_info",
    "16": "Alta a hospital federal",
    "17": "Alta a otra instituci√≥n",
    "18": "Alta a custodia policial",
    "19": "Sin_info",
    "20": "Alta por orden judicial",
    "21": "Sin_info",
    "22": "Falleci√≥ en casa",
    "23": "Falleci√≥ en instalaci√≥n m√©dica",
    "24": "Falleci√≥ en lugar desconocido",
    "25": "Falleci√≥ en cuidados paliativos",
    "28": "Falleci√≥ en centro de enfermer√≠a especializada"
}

map_admission_source = {
    "1": "Referencia m√©dica",
    "2": "Referencia desde cl√≠nica",
    "3": "Referencia desde aseguradora HMO",
    "4": "Transferencia desde hospital",
    "5": "Transferencia desde centro de enfermer√≠a especializada",
    "6": "Transferencia desde otro centro de salud",
    "7": "Sala de emergencias",
    "8": "Corte o custodia policial",
    "9": "Sin_info",
    "10": "Transferencia desde hospital de acceso cr√≠tico",
    "11": "Parto normal",
    "12": "Sin_info",
    "13": "Nacido en hospital",
    "14": "Nacido fuera de hospital",
    "15": "Sin_info",
    "17": "Sin_info",
    "20": "Sin_info",
    "22": "Sin_info",
    "25": "Sin_info"
}

# --- 3. Reemplazar con map() y 'Desconocido' para valores fuera del diccionario ---
df["admission_type_id"] = df["admission_type_id"].map(lambda x: map_admission_type.get(x, "Desconocido"))
df["discharge_disposition_id"] = df["discharge_disposition_id"].map(lambda x: map_discharge_disposition.get(x, "Desconocido"))
df["admission_source_id"] = df["admission_source_id"].map(lambda x: map_admission_source.get(x, "Desconocido"))

# --- 4. Verificaci√≥n r√°pida ---
st.write(df[["admission_type_id", "discharge_disposition_id", "admission_source_id"]].head())

# Tama√±o y forma del Dataset
st.write("Shape:", df.shape)
st.write("N√∫mero de filas:", df.shape[0])
st.write("N√∫mero de columnas:", df.shape[1])

# Mostrar informaci√≥n general del DataFrame (ADAPTACI√ìN)
st.write("--- Informaci√≥n del DataFrame ---")
buffer = io.StringIO()
df.info(buf=buffer)
st.text(buffer.getvalue())

# Revisar valores faltantes
st.write(df.isnull().sum())

faltantes = df.isnull().mean() * 100
st.write(faltantes.sort_values(ascending=False))

# Para columnas categ√≥ricas
for col in df.select_dtypes(include='object'):
    st.write(f"\nValores √∫nicos en {col}:")
    st.write(df[col].value_counts())

# Revisar duplicados
st.write("Duplicados:", df.duplicated().sum())

# Ver las filas duplicadas
st.write(df[df.duplicated()])

missing_vals = ["None", "?"]
df = df.replace(missing_vals, pd.NA)

# Verificar resultado en algunas columnas
st.write(df['max_glu_serum'].value_counts(dropna=False))
st.write(df['weight'].value_counts(dropna=False))

faltantes = df.isna().sum().sort_values(ascending=False)
faltantes_pct = (df.isna().mean() * 100).sort_values(ascending=False)

faltantes_df = pd.DataFrame({
    'Faltantes': faltantes,
    'Porcentaje': faltantes_pct
})
st.write(faltantes_df)

cat_cols = df.select_dtypes(include='object').columns

for col in cat_cols:
    if df[col].isna().sum() > 0:
        df[col] = df[col].fillna("Sin_info")
st.write(df[cat_cols].isna().sum().sort_values(ascending=False))

num_cols = df.select_dtypes(include=['int64','float64']).columns
st.write(df[num_cols].isna().sum().sort_values(ascending=False))

st.write(df)

df = df.drop(columns=["encounter_id", "patient_nbr"])

#Variables num√©ricas (PCA)
num_cols_pca = [
    "time_in_hospital", "num_lab_procedures", "num_procedures",
    "num_medications", "number_outpatient", "number_emergency",
    "number_inpatient", "number_diagnoses"
]

#Variables categ√≥ricas (MCA)
cat_cols_mca = [
    "race", "gender", "age", "weight", "payer_code",
    "medical_specialty", "diag_1", "diag_2", "diag_3",
    "max_glu_serum", "A1Cresult",
    "metformin", "repaglinide", "nateglinide", "chlorpropamide",
    "glimepiride", "acetohexamide", "glipizide", "glyburide",
    "tolbutamide", "pioglitazone", "rosiglitazone", "acarbose",
    "miglitol", "troglitazone", "tolazamide", "examide",
    "citoglipton", "insulin", "glyburide-metformin",
    "glipizide-metformin", "glimepiride-pioglitazone",
    "metformin-rosiglitazone", "metformin-pioglitazone",
    "change", "diabetesMed"
]

#Categ√≥ricas que estan con c√≥digos num√©ricos
categoricas_codigos = [
    "admission_type_id", "discharge_disposition_id", "admission_source_id"
]

#  las tenemos en cuenta para al MCA
cat_cols_mca.extend(categoricas_codigos)

# Verificaci√≥n
st.write("PCA (num√©ricas):", len(num_cols_pca), num_cols_pca)
st.write("MCA (categ√≥ricas):", len(cat_cols_mca), cat_cols_mca)

st.markdown("""üìäDistribuci√≥n de la variable objetivo (readmitted)""")

st.write(df["readmitted"].value_counts())

# Gr√°fico de barras de readmitted
fig, ax = plt.subplots(figsize=(6,4))
df["readmitted"].value_counts().plot(
    kind="bar",
    ax=ax,
    color=["#66c2a5","#fc8d62","#8da0cb"]
)
ax.set_title("Distribuci√≥n de readmisiones")
ax.set_ylabel("N√∫mero de pacientes")
ax.set_xlabel("Readmisi√≥n")
st.pyplot(fig)

st.markdown("""üìä Histograma / KDE de variables num√©ricas

Las num√©ricas relevantes

time_in_hospital: la mayor√≠a de pacientes pasan cuantos d√≠as?

num_lab_procedures: algunos pacientes tienen muchos m√°s ex√°menes que otros?

num_medications: hay tendencia a medicar m√°s a los que reingresan?
""")

num_cols = ["time_in_hospital", "num_lab_procedures", "num_medications"]
# Histograma m√∫ltiple
axes = df[num_cols].hist(bins=30, figsize=(12,6), edgecolor="black")
plt.suptitle("Distribuci√≥n de variables num√©ricas")
st.pyplot(plt.gcf())

st.markdown("""üìä Boxplot por clase objetivo""")

import seaborn as sns  # (ya importado arriba, lo mantengo como en tu c√≥digo)
# Boxplot
fig, ax = plt.subplots(figsize=(8,5))
sns.boxplot(x=df["readmitted"], y=df["time_in_hospital"], palette="Set3", ax=ax)
ax.set_title("Tiempo en hospital seg√∫n readmisi√≥n")
ax.set_xlabel("Readmisi√≥n")
ax.set_ylabel("Tiempo en hospital (d√≠as)")
st.pyplot(fig)

st.markdown("""üìä Distribucion de g√©nero o raza""")

fig, ax = plt.subplots(figsize=(6,4))
sns.countplot(x=df["gender"], palette="pastel", ax=ax)
ax.set_title("Distribuci√≥n por g√©nero")
st.pyplot(fig)

fig, ax = plt.subplots(figsize=(8,5))
sns.countplot(x=df["race"], palette="muted", ax=ax)
ax.set_title("Distribuci√≥n por raza")
plt.xticks(rotation=45)
st.pyplot(fig)

st.markdown("""üìä Mapa de calor de correlaciones (solo num√©ricas)""")

fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(df[num_cols_pca].corr(), annot=True, cmap="coolwarm", fmt=".2f", ax=ax)
ax.set_title("Matriz de correlaci√≥n - variables num√©ricas")
st.pyplot(fig)

x= df.drop("readmitted", axis=1)
y= df["readmitted"]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test  = train_test_split(x,y, test_size=0.2, stratify=y, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train[num_cols_pca])
X_test_scaled  = scaler.transform(x_test[num_cols_pca])

st.markdown("""# TAREA 1: PCA + MCA""")

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler as _SS_  # se mantiene import redundante como en tu c√≥digo
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt as _plt_  # (mantener idea de imports m√∫ltiples)
import seaborn as sns as _sns_
import numpy as np as _np_
import pandas as pd as _pd_

# PCA con todos los componentes
pca = PCA()
X_pca = pca.fit_transform(X_train_scaled)

# Varianza acumulada
explained_var = np.cumsum(pca.explained_variance_ratio_)

fig, ax = plt.subplots(figsize=(8,5))
ax.plot(range(1, len(explained_var) + 1), explained_var, marker='o', linestyle='--')
ax.axhline(y=0.85, color='r', linestyle='-')
ax.set_xlabel('N√∫mero de componentes principales')
ax.set_ylabel('Varianza acumulada explicada')
ax.set_title('Varianza acumulada explicada por PCA')
ax.grid(True)
st.pyplot(fig)

# Scatterplot PC1 vs PC2
fig, ax = plt.subplots(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_train, palette='Set1', alpha=0.7, ax=ax)
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_title('Scatterplot PC1 vs PC2')
ax.legend(title='Clase', labels=['NO', '<30', '>30'])
st.pyplot(fig)

# Loadings (cargas) de las variables en las PCs
loadings = pd.DataFrame(pca.components_.T,columns=[f'PC{i+1}' for i in range(pca.n_components_)],index=num_cols_pca)

fig, ax = plt.subplots(figsize=(12,8))
sns.heatmap(loadings.iloc[:,:10], annot=True, cmap='coolwarm', center=0, ax=ax)
ax.set_title('Heatmap de loadings (primeras PCs)')
st.pyplot(fig)

# Aplicar PCA
pca = PCA(n_components=0.85)
X_pca = pca.fit_transform(X_train_scaled)

st.write(f"N√∫mero de componentes principales para explicar 85% varianza: {pca.n_components_}")
st.write(f"Varianza explicada acumulada por estas componentes: {sum(pca.explained_variance_ratio_):.4f}")

# (ADAPTACI√ìN) Instalar paquetes en runtime no es viable; import con manejo de error
try:
    import mca
except Exception:
    st.warning("El paquete 'mca' no est√° instalado (no se usa directamente m√°s adelante).")

try:
    import prince
except Exception as e:
    st.error("El paquete 'prince' es requerido para MCA. Agr√©galo a requirements.txt.")
    st.stop()

import pandas as pd  # repetidos en tu c√≥digo original
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# from sklearn.datasets import load_breast_cancer  # ya importado arriba

X_train_cat = x_train[cat_cols_mca].astype(str)
X_test_cat  = x_test[cat_cols_mca].astype(str)

# Aplicar MCA
x_mca = prince.MCA(n_components=15,random_state=42)
x_mca = x_mca.fit(X_train_cat)

#Transformar dimensiones MCA
X_mca = x_mca.transform(X_train_cat)

#Varianza explicada acumulada
eigvals = x_mca.eigenvalues_
var_exp = eigvals / eigvals.sum()
cum_var_exp = np.cumsum(var_exp)

# Graficar varianza acumulada
fig, ax = plt.subplots(figsize=(8,5))
ax.plot(range(1, len(cum_var_exp)+1), cum_var_exp, marker='o', linestyle='--')
ax.axhline(y=0.85, color='r', linestyle='-')
ax.set_xlabel('Dimensiones MCA')
ax.set_ylabel('Varianza acumulada explicada')
ax.set_title('Varianza acumulada explicada por MCA')
ax.grid(True)
st.pyplot(fig)

import matplotlib.pyplot as plt
import seaborn as sns

# Coordenadas de las primeras 2 dimensiones
loadings_cat = x_mca.column_coordinates(X_train_cat).iloc[:, :2]

# Calcular contribuci√≥n por categor√≠a
loadings_sq = loadings_cat ** 2
contrib_cat = loadings_sq.div(loadings_sq.sum(axis=0), axis=1)

# Extraer nombre de variable original (asumiendo formato 'variable__categor√≠a')
contrib_cat.index = contrib_cat.index.str.split('__').str[0]

# Sumar contribuciones por variable
contrib_var = contrib_cat.groupby(contrib_cat.index).sum().sum(axis=1)

# Normalizar para mostrar como porcentaje
contrib_pct = contrib_var / contrib_var.sum() * 100

# Ordenar
contrib_pct_sorted = contrib_pct.sort_values()

# Graficar horizontalmente
fig, ax = plt.subplots(figsize=(12, 10))
sns.barplot(x=contrib_pct_sorted.values, y=contrib_pct_sorted.index, palette='BuGn', ax=ax)
ax.set_ylabel('Contribuci√≥n (%) a Dim 1 y 2')
ax.set_title('Contribuci√≥n total de todas las variables a las primeras 2 dimensiones MCA')
ax.grid(axis='x', linestyle='--', alpha=0.5)

# Agregar etiquetas a cada barra
for i, v in enumerate(contrib_pct_sorted.values):
    ax.text(v + 0.2, i, f"{v:.2f}%", va='center')

plt.tight_layout()
st.pyplot(fig)

import numpy as np
X_reduced = np.hstack((X_pca, X_mca.values))

#Seleccionar componentes seg√∫n varianza acumulada ---
n_pca = np.argmax(explained_var >= 0.85) + 1
X_pca_reduced = X_pca[:, :n_pca]

n_mca = np.argmax(cum_var_exp >= 0.85) + 1
X_mca_reduced = X_mca.iloc[:, :n_mca].values  # convertir a numpy array

#Concatenar
X_reduced = np.hstack((X_pca_reduced, X_mca_reduced))

# Dataframe
pca_col_names = [f"PCA_{i+1}" for i in range(n_pca)]
mca_col_names = [f"MCA_{i+1}" for i in range(n_mca)]
col_names = pca_col_names + mca_col_names

X_reduced_df = pd.DataFrame(X_reduced, columns=col_names, index=x_train.index)

st.write(X_reduced_df.head())

st.write("Filas X_train:", x_train.shape[0])
st.write("Filas PCA:", X_pca.shape[0])
st.write("Filas MCA:", X_mca.shape[0])
st.write("Filas X_reduced_df:", X_reduced_df.shape[0])
st.write("N√∫mero de filas:", X_reduced_df.shape[0])
st.write("N√∫mero de columnas:", X_reduced_df.shape[1])
st.write("√çndices iguales?", X_reduced_df.index.equals(y_train.index))

st.markdown("""# An√°lisis de los resultados obtenidos en MCA y PCA

üîπ An√°lisis de resultados PCA

Al aplicar PCA sobre las variables num√©ricas del dataset:

Se observ√≥ que con 6 componentes principales se logra conservar la mayor parte de la informaci√≥n sin necesidad de mantener todas las variables originales, lo que significa que el espacio de alta dimensionalidad de las variables num√©ricas (como valores de laboratorio, n√∫mero de visitas, edad, etc.) puede representarse en un espacio reducido, facilitando el an√°lisis.

El scatterplot mostr√≥ cierta separaci√≥n entre las clases de readmisi√≥n (NO, <30, >30), aunque con solapamiento. Esto confirma que existen patrones num√©ricos que ayudan a explicar parte de la variabilidad entre pacientes, pero no son totalmente discriminantes por s√≠ solos.

Conclusi√≥n
La reducci√≥n permiti√≥ simplificar el conjunto de variables num√©ricas manteniendo m√°s del 85% de la varianza. Esto mejora la eficiencia y reduce el riesgo de sobreajuste, aunque por s√≠ sola la variabilidad num√©rica no separa completamente las clases.

üîπ An√°lisis de resultados MCA

Con el MCA aplicado a las variables categ√≥ricas (como g√©nero, tipo de admisi√≥n, tipo de alta, diagn√≥sticos, etc.):

Se encontr√≥ que con aproximadamente 13 dimensiones se logra superar el 85% de varianza acumulada, indicando que gran parte de la informaci√≥n categ√≥rica puede concentrarse en un espacio reducido.

El MCA permiti√≥ visualizar la relaci√≥n entre categor√≠as: por ejemplo, algunos tipos de admisi√≥n o de disposici√≥n al alta tienden a asociarse con mayor probabilidad de reingreso.


üîπ Integraci√≥n PCA + MCA

Al combinar los componentes num√©ricos (PCA) y categ√≥ricos (MCA):

Se obtiene un conjunto de datos reducido y balanceado entre ambos tipos de informaci√≥n, manteniendo la mayor parte de la varianza de los datos originales.

#KNN Y ARBOL
""")

X_reduced_df['target'] = y_train.values
st.write(X_reduced_df.head())

X_final = X_reduced_df.drop('target', axis=1)
y_final = X_reduced_df['target']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_Scaled = scaler.fit_transform(X_final)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_final_encoded = le.fit_transform(y_final)

X_train, X_test, y_train, y_test = train_test_split(
    X_Scaled,y_final_encoded,test_size=0.2,random_state=42,stratify=y_final_encoded
)

st.write(pd.Series(y_train).value_counts().sort_index())

import matplotlib.pyplot as plt
import pandas as pd

y_series = pd.Series(y_train)
# Contar las clases
counts = y_series.value_counts().sort_index()

# Etiquetas que quieras mostrar
labels = ['NO', '<30', '>30']

# Graficar
fig, ax = plt.subplots(figsize=(6,4))
ax.bar(labels, counts)
ax.set_xlabel('Clase')
ax.set_ylabel('Cantidad de observaciones')
ax.set_title('Distribuci√≥n de la variable objetivo')
st.pyplot(fig)

st.markdown("""#**Desbalanceo de Datos**""")

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

st.write(pd.Series(y_train_res).value_counts().sort_index())


