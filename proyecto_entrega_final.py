# -*- coding: utf-8 -*-
"""Proyecto Entrega 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KuGCEkQzUpyxk6nJE2k5TqbylEbhblw3

##**Diabetes-Datos cl√≠nicos de 130 hospitales y centros de atenci√≥n integrados en EE.UU**

El conjunto de datos:
Readmisiones hospitalarias por diabetesde atenci√≥n cl√≠nica en 130 hospitales de EE. UU. y redes de prestaci√≥n integradas.
Periodo: 10 a√±os (1999‚Äì2008)


Los datos contienen atributos como el n√∫mero de pacientes, la raza, el sexo, la edad, el tipo de ingreso, el tiempo en el hospital, la especialidad m√©dica del m√©dico de admisi√≥n, el n√∫mero de pruebas de laboratorio realizadas, el resultado de la prueba de HbA1c, el diagn√≥stico, el n√∫mero de medicamentos, los medicamentos para la diabetes, el n√∫mero de visitas ambulatorias, hospitalarias y de emergencia en el a√±o anterior a la hospitalizaci√≥n, etc.

Objetivo: Analizar la readmisi√≥n de pacientes diab√©ticos, es decir, si despu√©s del alta regresan al hospital dentro de 30 d√≠as, despu√©s de 30 d√≠as o nunca

---
---
# **1. Obtener los datos**

---
---
"""

# example of chi squared feature selection for categorical data
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from matplotlib import pyplot

#Montar Googgle Drive y mover los archivos ah√≠
#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("diabetic_data.csv")
df.head()

# Ver valores √∫nicos en las columnas categ√≥ricas codificadas
print("admission_type_id:", sorted(df["admission_type_id"].unique()))
print("discharge_disposition_id:", sorted(df["discharge_disposition_id"].unique()))
print("admission_source_id:", sorted(df["admission_source_id"].unique()))

#Convertir a string
df["admission_type_id"] = df["admission_type_id"].astype(str)
df["discharge_disposition_id"] = df["discharge_disposition_id"].astype(str)
df["admission_source_id"] = df["admission_source_id"].astype(str)

map_admission_type = {
    "1": "Emergencia",
    "2": "Urgente",
    "3": "Electivo",
    "4": "Reci√©n nacido",
    "5": "Sin_info",
    "6": "Sin_info",
    "7": "Centro de trauma",
    "8": "Sin_info"
}

map_discharge_disposition = {
    "1": "Alta a casa",
    "2": "Alta a otro hospital a corto plazo",
    "3": "Alta a centro de enfermer√≠a especializada",
    "4": "Alta a centro de cuidados intermedios",
    "5": "Alta a otro tipo de atenci√≥n hospitalaria",
    "6": "Cuidados de salud en casa",
    "7": "Salida contra recomendaci√≥n m√©dica",
    "8": "Alta a casa con cuidados",
    "9": "Admitido como paciente hospitalizado",
    "10": "Cuidados paliativos en casa",
    "11": "Cuidados paliativos en centro m√©dico",
    "12": "Alta a hospital psiqui√°trico",
    "13": "Alta a otra instalaci√≥n de rehabilitaci√≥n",
    "14": "Sin_info",
    "15": "Sin_info",
    "16": "Alta a hospital federal",
    "17": "Alta a otra instituci√≥n",
    "18": "Alta a custodia policial",
    "19": "Sin_info",
    "20": "Alta por orden judicial",
    "21": "Sin_info",
    "22": "Falleci√≥ en casa",
    "23": "Falleci√≥ en instalaci√≥n m√©dica",
    "24": "Falleci√≥ en lugar desconocido",
    "25": "Falleci√≥ en cuidados paliativos",
    "28": "Falleci√≥ en centro de enfermer√≠a especializada"
}

map_admission_source = {
    "1": "Referencia m√©dica",
    "2": "Referencia desde cl√≠nica",
    "3": "Referencia desde aseguradora HMO",
    "4": "Transferencia desde hospital",
    "5": "Transferencia desde centro de enfermer√≠a especializada",
    "6": "Transferencia desde otro centro de salud",
    "7": "Sala de emergencias",
    "8": "Corte o custodia policial",
    "9": "Sin_info",
    "10": "Transferencia desde hospital de acceso cr√≠tico",
    "11": "Parto normal",
    "12": "Sin_info",
    "13": "Nacido en hospital",
    "14": "Nacido fuera de hospital",
    "15": "Sin_info",
    "17": "Sin_info",
    "20": "Sin_info",
    "22": "Sin_info",
    "25": "Sin_info"

}

# --- 3. Reemplazar con map() y 'Desconocido' para valores fuera del diccionario ---
df["admission_type_id"] = df["admission_type_id"].map(lambda x: map_admission_type.get(x, "Desconocido"))
df["discharge_disposition_id"] = df["discharge_disposition_id"].map(lambda x: map_discharge_disposition.get(x, "Desconocido"))
df["admission_source_id"] = df["admission_source_id"].map(lambda x: map_admission_source.get(x, "Desconocido"))

# --- 4. Verificaci√≥n r√°pida ---
print(df[["admission_type_id", "discharge_disposition_id", "admission_source_id"]].head())

# Tama√±o y forma del Dataset
print("Shape:", df.shape)
print("N√∫mero de filas:", df.shape[0])
print("N√∫mero de columnas:", df.shape[1])

# Mostrar informaci√≥n general del DataFrame
print("--- Informaci√≥n del DataFrame ---")
df.info()

# Revisar valores faltantes
df.isnull().sum()

faltantes = df.isnull().mean() * 100
print(faltantes.sort_values(ascending=False))

# Para columnas categ√≥ricas
for col in df.select_dtypes(include='object'):
    print(f"\nValores √∫nicos en {col}:")
    print(df[col].value_counts())

# Revisar duplicados
print("Duplicados:", df.duplicated().sum())

# Ver las filas duplicadas
df[df.duplicated()]

missing_vals = ["None", "?"]
df = df.replace(missing_vals, pd.NA)

# Verificar resultado en algunas columnas
print(df['max_glu_serum'].value_counts(dropna=False))
print(df['weight'].value_counts(dropna=False))

faltantes = df.isna().sum().sort_values(ascending=False)
faltantes_pct = (df.isna().mean() * 100).sort_values(ascending=False)

faltantes_df = pd.DataFrame({
    'Faltantes': faltantes,
    'Porcentaje': faltantes_pct
})
print(faltantes_df)

cat_cols = df.select_dtypes(include='object').columns

for col in cat_cols:
    if df[col].isna().sum() > 0:
        df[col] = df[col].fillna("Sin_info")
print(df[cat_cols].isna().sum().sort_values(ascending=False))

num_cols = df.select_dtypes(include=['int64','float64']).columns
print(df[num_cols].isna().sum().sort_values(ascending=False))

df

df = df.drop(columns=["encounter_id", "patient_nbr"])

#Variables num√©ricas (PCA)
num_cols_pca = [
    "time_in_hospital", "num_lab_procedures", "num_procedures",
    "num_medications", "number_outpatient", "number_emergency",
    "number_inpatient", "number_diagnoses"
]

#Variables categ√≥ricas (MCA)
cat_cols_mca = [
    "race", "gender", "age", "weight", "payer_code",
    "medical_specialty", "diag_1", "diag_2", "diag_3",
    "max_glu_serum", "A1Cresult",
    "metformin", "repaglinide", "nateglinide", "chlorpropamide",
    "glimepiride", "acetohexamide", "glipizide", "glyburide",
    "tolbutamide", "pioglitazone", "rosiglitazone", "acarbose",
    "miglitol", "troglitazone", "tolazamide", "examide",
    "citoglipton", "insulin", "glyburide-metformin",
    "glipizide-metformin", "glimepiride-pioglitazone",
    "metformin-rosiglitazone", "metformin-pioglitazone",
    "change", "diabetesMed"
]

#Categ√≥ricas que estan con c√≥digos num√©ricos
categoricas_codigos = [
    "admission_type_id", "discharge_disposition_id", "admission_source_id"
]

#  las tenemos en cuenta para al MCA
cat_cols_mca.extend(categoricas_codigos)

# Verificaci√≥n
print("PCA (num√©ricas):", len(num_cols_pca), num_cols_pca)
print("MCA (categ√≥ricas):", len(cat_cols_mca), cat_cols_mca)

"""üìäDistribuci√≥n de la variable objetivo (readmitted)"""

df["readmitted"].value_counts()

plt.figure(figsize=(6,4))
sns.countplot(x=y, order=y.value_counts().index, palette="Set2")
plt.title("Distribuci√≥n de readmisiones")
plt.ylabel("N√∫mero de pacientes")
plt.xlabel("Readmisi√≥n")
plt.show()

"""üìä Histograma / KDE de variables num√©ricas

Las num√©ricas relevantes

time_in_hospital: la mayor√≠a de pacientes pasan cuantos d√≠as?

num_lab_procedures: algunos pacientes tienen muchos m√°s ex√°menes que otros?

num_medications: hay tendencia a medicar m√°s a los que reingresan?
"""

num_cols = ["time_in_hospital", "num_lab_procedures", "num_medications"]
df[num_cols].hist(bins=30, figsize=(12,6), edgecolor="black")
plt.suptitle("Distribuci√≥n de variables num√©ricas")
plt.show()

"""üìä Boxplot por clase objetivo"""

plt.figure(figsize=(8,5))
sns.boxplot(x=y, y=df["time_in_hospital"], palette="Set3")
plt.title("Tiempo en hospital seg√∫n readmisi√≥n")
plt.show()

"""üìä Distribucion de g√©nero o raza"""

plt.figure(figsize=(6,4))
sns.countplot(x=df["gender"], palette="pastel")
plt.title("Distribuci√≥n por g√©nero")
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(x=df["race"], palette="muted")
plt.title("Distribuci√≥n por raza")
plt.xticks(rotation=45)
plt.show()

"""üìä Mapa de calor de correlaciones (solo num√©ricas)"""

plt.figure(figsize=(10,8))
sns.heatmap(df[num_cols_pca].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matriz de correlaci√≥n - variables num√©ricas")
plt.show()

x= df.drop("readmitted", axis=1)
y= df["readmitted"]

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test  = train_test_split(x,y, test_size=0.2, stratify=y, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train[num_cols_pca])
X_test_scaled  = scaler.transform(x_test[num_cols_pca])

"""# TAREA 1: PCA + MCA"""

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd


# PCA con todos los componentes
pca = PCA()
X_pca = pca.fit_transform(X_train_scaled)

# Varianza acumulada
explained_var = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_var) + 1), explained_var, marker='o', linestyle='--')
plt.axhline(y=0.85, color='r', linestyle='-')
plt.xlabel('N√∫mero de componentes principales')
plt.ylabel('Varianza acumulada explicada')
plt.title('Varianza acumulada explicada por PCA')
plt.grid(True)
plt.show()

# Scatterplot PC1 vs PC2
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_train, palette='Set1', alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Scatterplot PC1 vs PC2')
plt.legend(title='Clase', labels=['NO', '<30', '>30'])
plt.show()

# Loadings (cargas) de las variables en las PCs
loadings = pd.DataFrame(pca.components_.T,columns=[f'PC{i+1}' for i in range(pca.n_components_)],index=num_cols_pca)

plt.figure(figsize=(12,8))
sns.heatmap(loadings.iloc[:,:10], annot=True, cmap='coolwarm', center=0)
plt.title('Heatmap de loadings (primeras PCs)')
plt.show()

# Aplicar PCA
pca = PCA(n_components=0.85)
X_pca = pca.fit_transform(X_train_scaled)

print(f"N√∫mero de componentes principales para explicar 85% varianza: {pca.n_components_}")
print(f"Varianza explicada acumulada por estas componentes: {sum(pca.explained_variance_ratio_):.4f}")

#!pip install mca

#!pip install prince

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import mca
import prince
from sklearn.datasets import load_breast_cancer

X_train_cat = x_train[cat_cols_mca].astype(str)
X_test_cat  = x_test[cat_cols_mca].astype(str)

# Aplicar MCA
x_mca = prince.MCA(n_components=15,random_state=42)
x_mca = x_mca.fit(X_train_cat)

#Transformar dimensiones MCA
X_mca = x_mca.transform(X_train_cat)

#Varianza explicada acumulada
eigvals = x_mca.eigenvalues_
var_exp = eigvals / eigvals.sum()
cum_var_exp = np.cumsum(var_exp)

# Graficar varianza acumulada
plt.figure(figsize=(8,5))
plt.plot(range(1, len(cum_var_exp)+1), cum_var_exp, marker='o', linestyle='--')
plt.axhline(y=0.85, color='r', linestyle='-')
plt.xlabel('Dimensiones MCA')
plt.ylabel('Varianza acumulada explicada')
plt.title('Varianza acumulada explicada por MCA')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Coordenadas de las primeras 2 dimensiones
loadings_cat = x_mca.column_coordinates(X_train_cat).iloc[:, :2]

# Calcular contribuci√≥n por categor√≠a
loadings_sq = loadings_cat ** 2
contrib_cat = loadings_sq.div(loadings_sq.sum(axis=0), axis=1)

# Extraer nombre de variable original (asumiendo formato 'variable__categor√≠a')
contrib_cat.index = contrib_cat.index.str.split('__').str[0]

# Sumar contribuciones por variable
contrib_var = contrib_cat.groupby(contrib_cat.index).sum().sum(axis=1)

# Normalizar para mostrar como porcentaje
contrib_pct = contrib_var / contrib_var.sum() * 100

# Ordenar
contrib_pct_sorted = contrib_pct.sort_values()

# Graficar horizontalmente
plt.figure(figsize=(12, 10))
sns.barplot(x=contrib_pct_sorted.values, y=contrib_pct_sorted.index, palette='BuGn')
plt.ylabel('Contribuci√≥n (%) a Dim 1 y 2')
plt.title('Contribuci√≥n total de todas las variables a las primeras 2 dimensiones MCA')
plt.grid(axis='x', linestyle='--', alpha=0.5)

# Agregar etiquetas a cada barra
for i, v in enumerate(contrib_pct_sorted.values):
    plt.text(v + 0.2, i, f"{v:.2f}%", va='center')

plt.tight_layout()
plt.show()

import numpy as np
X_reduced = np.hstack((X_pca, X_mca.values))

#Seleccionar componentes seg√∫n varianza acumulada ---
n_pca = np.argmax(explained_var >= 0.85) + 1
X_pca_reduced = X_pca[:, :n_pca]

n_mca = np.argmax(cum_var_exp >= 0.85) + 1
X_mca_reduced = X_mca.iloc[:, :n_mca].values  # convertir a numpy array

#Concatenar
X_reduced = np.hstack((X_pca_reduced, X_mca_reduced))

# Dataframe
pca_col_names = [f"PCA_{i+1}" for i in range(n_pca)]
mca_col_names = [f"MCA_{i+1}" for i in range(n_mca)]
col_names = pca_col_names + mca_col_names

X_reduced_df = pd.DataFrame(X_reduced, columns=col_names, index=x_train.index)


X_reduced_df.head()

print("Filas X_train:", x_train.shape[0])
print("Filas PCA:", X_pca.shape[0])
print("Filas MCA:", X_mca.shape[0])
print("Filas X_reduced_df:", X_reduced_df.shape[0])
print("N√∫mero de filas:", X_reduced_df.shape[0])
print("N√∫mero de columnas:", X_reduced_df.shape[1])
print("√çndices iguales?", X_reduced_df.index.equals(y_train.index))

"""# An√°lisis de los resultados obtenidos en MCA y PCA

üîπ An√°lisis de resultados PCA

Al aplicar PCA sobre las variables num√©ricas del dataset:

Se observ√≥ que con 6 componentes principales se logra conservar la mayor parte de la informaci√≥n sin necesidad de mantener todas las variables originales, lo que significa que el espacio de alta dimensionalidad de las variables num√©ricas (como valores de laboratorio, n√∫mero de visitas, edad, etc.) puede representarse en un espacio reducido, facilitando el an√°lisis.

El scatterplot mostr√≥ cierta separaci√≥n entre las clases de readmisi√≥n (NO, <30, >30), aunque con solapamiento. Esto confirma que existen patrones num√©ricos que ayudan a explicar parte de la variabilidad entre pacientes, pero no son totalmente discriminantes por s√≠ solos.

Conclusi√≥n
La reducci√≥n permiti√≥ simplificar el conjunto de variables num√©ricas manteniendo m√°s del 85% de la varianza. Esto mejora la eficiencia y reduce el riesgo de sobreajuste, aunque por s√≠ sola la variabilidad num√©rica no separa completamente las clases.

üîπ An√°lisis de resultados MCA

Con el MCA aplicado a las variables categ√≥ricas (como g√©nero, tipo de admisi√≥n, tipo de alta, diagn√≥sticos, etc.):

Se encontr√≥ que con aproximadamente 13 dimensiones se logra superar el 85% de varianza acumulada, indicando que gran parte de la informaci√≥n categ√≥rica puede concentrarse en un espacio reducido.

El MCA permiti√≥ visualizar la relaci√≥n entre categor√≠as: por ejemplo, algunos tipos de admisi√≥n o de disposici√≥n al alta tienden a asociarse con mayor probabilidad de reingreso.


üîπ Integraci√≥n PCA + MCA

Al combinar los componentes num√©ricos (PCA) y categ√≥ricos (MCA):

Se obtiene un conjunto de datos reducido y balanceado entre ambos tipos de informaci√≥n, manteniendo la mayor parte de la varianza de los datos originales.

#KNN Y ARBOL
"""

X_reduced_df['target'] = y_train.values
X_reduced_df.head()

X_final = X_reduced_df.drop('target', axis=1)
y_final = X_reduced_df['target']

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_Scaled = scaler.fit_transform(X_final)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_final_encoded = le.fit_transform(y_final)

X_train, X_test, y_train, y_test = train_test_split(
    X_Scaled,y_final_encoded,test_size=0.2,random_state=42,stratify=y_final_encoded
)

pd.Series(y_train).value_counts().sort_index()

import matplotlib.pyplot as plt
import pandas as pd

y_series = pd.Series(y_train)
# Contar las clases
counts = y_series.value_counts().sort_index()

# Etiquetas que quieras mostrar
labels = ['NO', '<30', '>30']

# Graficar
plt.figure(figsize=(6,4))
plt.bar(labels, counts)
plt.xlabel('Clase')
plt.ylabel('Cantidad de observaciones')
plt.title('Distribuci√≥n de la variable objetivo')
plt.show()

"""#**Desbalanceo de Datos**"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
